# -*- coding: utf-8 -*-
"""logistic_regression_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fz8VYTfa5_xoQ8lAnlt926L5uJlT-oR-
"""

import mysql.connector
import sshtunnel
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import gc
import warnings
warnings.filterwarnings("ignore")

sshtunnel.SSH_TIMEOUT = 5.0
sshtunnel.TUNNEL_TIMEOUT = 5.0

with sshtunnel.SSHTunnelForwarder(
    ('ssh.pythonanywhere.com'),
    ssh_username='Gymlab', ssh_password='B@s3C@mp!',
    remote_bind_address=('Gymlab.mysql.pythonanywhere-services.com', 3306)
) as tunnel:
    connection = mysql.connector.connect(
        user='Gymlab', password='B@s3C@mp!',
        host='127.0.0.1', port=tunnel.local_bind_port,
        database='Gymlab$pydb',
    )
    
    table = pd.read_sql('SELECT * FROM data_goalie_prio', connection)
    
    connection.close()

# Load first five rows
table.head(5)

# Check for duplicate rows by id/remove columns/missing values to 0
table.sort_values("id",inplace=True)
table.drop_duplicates(subset="id",keep=False,inplace=True)
del table["date_from"]
del table["date_to"]
del table["id"]
table.loc[table['gender'] == "f", 'gender'] = 1
table.loc[table['gender'] == "m", 'gender'] = 0
table.loc[table['gender'] == "M", 'gender'] = 0
table = table.fillna(0)
table.head(5)

# Move target column at the end
table = table[["member_id","yearweek","year","month","week_number","member_since_weeks","member_since_months",
               "member_since_years","age","gender","visit_count_m4_wk","visit_count_m3_wk","visit_count_m2_wk",
               "visit_count_m1_wk","visit_count_0_wk","visit_count_m1_4_wk_mean","visit_count_m1_4_wk_stdev",
               "visit_count_1_wk"]]
table.head(5)

# Rename columns
table["first_week"] = table["visit_count_m4_wk"]
table["second_week"] = table["visit_count_m3_wk"]
table["third_week"] = table["visit_count_m2_wk"]
table["fourth_week"] = table["visit_count_m1_wk"]
table["present_week"] = table["visit_count_0_wk"]
table["target_week"] = table["visit_count_1_wk"]
table["first_four_weeks_mean"] = table["visit_count_m1_4_wk_mean"]
table["first_four_weeks_std"] = table["visit_count_m1_4_wk_stdev"]
table.index = table.member_id
table = table[["first_week","second_week","third_week","fourth_week","present_week",
               "first_four_weeks_mean","first_four_weeks_std","target_week"]]
table.head(5)

# Classify Target Column
def f(row):
    if row["target_week"] >= 0.5:
        val = 1
    else:
        val = 0
    return val
table["target_week"] = table.apply(f, axis=1)
table.head(5)

"""Feature Selection"""

# Stratify target 
table_sample1 = table.loc[table.target_week==1].sample(frac=0.1, replace=False)
#print('label 1 sample size:', str(table_sample1.shape[0]))
table_sample0 = table.loc[table.target_week==0].sample(frac=0.1, replace=False)
#print('label 0 sample size:', str(table_sample0.shape[0]))
table = pd.concat([table_sample1, table_sample0], axis=0)

# input-output
X = table.drop(["target_week"], axis=1)
y = table.target_week
feature_name = X.columns.tolist()

# pearson correlation
def cor_selector(X, y):
    cor_list = []
    # calculate the correlation with y for each feature
    for i in X.columns.tolist():
        cor = np.corrcoef(X[i], y)[0, 1]
        cor_list.append(cor)
    # replace NaN with 0
    cor_list = [0 if np.isnan(i) else i for i in cor_list]
    # feature name
    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-100:]].columns.tolist()
    # feature selection? 0 for not select, 1 for select
    cor_support = [True if i in cor_feature else False for i in feature_name]
    return cor_support, cor_feature

cor_support, cor_feature = cor_selector(X, y)
#print(str(len(cor_feature)), 'selected features')

# chi-2
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.preprocessing import MinMaxScaler

X_norm = MinMaxScaler().fit_transform(X)
chi_selector = SelectKBest(chi2, k="all")
chi_selector.fit(X_norm, y)

chi_support = chi_selector.get_support()
chi_feature = X.loc[:,chi_support].columns.tolist()
#print(str(len(chi_feature)), 'selected features')

# Wrapper
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=100, step=10, verbose=5)
rfe_selector.fit(X_norm, y)

rfe_support = rfe_selector.get_support()
rfe_feature = X.loc[:,rfe_support].columns.tolist()
#print(str(len(rfe_feature)), 'selected features')

# Logistic regression
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression

embeded_lr_selector = SelectFromModel(LogisticRegression(penalty="l1"), '1.25*median')
embeded_lr_selector.fit(X_norm, y)

embeded_lr_support = embeded_lr_selector.get_support()
embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()
#print(str(len(embeded_lr_feature)), 'selected features')

# Random forest
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), threshold='1.25*median')
embeded_rf_selector.fit(X, y)

embeded_rf_support = embeded_rf_selector.get_support()
embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()
#print(str(len(embeded_rf_feature)), 'selected features')

from sklearn.feature_selection import SelectFromModel
from lightgbm import LGBMClassifier

lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,
            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)

embeded_lgb_selector = SelectFromModel(lgbc, threshold='1.25*median')
embeded_lgb_selector.fit(X, y)

embeded_lgb_support = embeded_lgb_selector.get_support()
embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()
#print(str(len(embeded_lgb_feature)), 'selected features')

# Summary
pd.set_option('display.max_rows', None)
# put all selection together
feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,
                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})
# count the selected times for each feature
feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)
# display the top 100
feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)
feature_selection_df.index = range(1, len(feature_selection_df)+1)
feature_selection_df.head(100)

# Preprocessing
X = table.iloc[:, :-1].values
y = table.iloc[:, -1].values

# Train-test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=0)

# Feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)
print('The accuracy of logistic regression classifier on test set is: {:.2f}'.format(logreg.score(X_test, y_test)))

from sklearn.metrics import confusion_matrix

confusion_matrix = confusion_matrix(y_test, y_pred)
#print(confusion_matrix)

from sklearn.metrics import classification_report

#print(classification_report(y_test, y_pred))

# Calculate the coefficients - intercept
betas = logreg.coef_
alpha = logreg.intercept_
#print(alpha)
#print(betas)

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

# Predict probabilities of class 0
zero = logreg.predict_proba(X)[:,0]
zero

# Predict probabilities of class 1
one = logreg.predict_proba(X)[:,1]
one

# Add probabilities' columns to table
table['zero'] = zero
table['one'] = one
table.head(5)

# Create dataframe
data = pd.DataFrame({
    'member_id':table.index,
    'weekly_churn_prob':table.zero,
    'weekly_retention_prob':table.one,
    'updated_at':"-"
})

data = data.drop_duplicates(subset='member_id', keep="first")
print(data.to_string(index=False))

# Fill ds_output table
import MySQLdb

conn = MySQLdb.connect(
    host='Gymlab.mysql.pythonanywhere-services.com',
    user='Gymlab',
    passwd='B@s3C@mp!',
    db='Gymlab$pydb')

cur = conn.cursor()

# creating column list for insertion
cols = "`,`".join([str(i) for i in data.columns.tolist()])

# Insert DataFrame recrds one by one.
for i,row in data.iterrows():
    sql = "INSERT INTO `ds_output` (`" +cols + "`) VALUES (" + "%s,"*(len(row)-1) + "%s) ON DUPLICATE KEY UPDATE weekly_churn_prob=VALUES(weekly_churn_prob), weekly_retention_prob=VALUES(weekly_retention_prob)"
    cur.execute(sql, tuple(row))

    # the connection is not autocommitted by default, so we must commit to save our changes
    conn.commit()

sshtunnel.SSH_TIMEOUT = 5.0
sshtunnel.TUNNEL_TIMEOUT = 5.0

with sshtunnel.SSHTunnelForwarder(
    ('ssh.pythonanywhere.com'),
    ssh_username='Gymlab', ssh_password='B@s3C@mp!',
    remote_bind_address=('Gymlab.mysql.pythonanywhere-services.com', 3306)
) as tunnel:
    connection = mysql.connector.connect(
        user='Gymlab', password='B@s3C@mp!',
        host='127.0.0.1', port=tunnel.local_bind_port,
        database='Gymlab$pydb',
    )
    
    new_table = pd.read_sql('SELECT * FROM dwh_members', connection)
    
    connection.close()

# Load new_table
new_table.head(5)

# Check for duplicate rows by id/remove columns/missing values to 0
new_table.sort_values("member_id",inplace=True)
new_table.drop_duplicates(subset="member_id",keep=False,inplace=True)
new_table.loc[new_table['gender'] == "f", 'gender'] = 1
new_table.loc[new_table['gender'] == "m", 'gender'] = 0
new_table.loc[new_table['gender'] == "M", 'gender'] = 0
new_table.index = new_table.member_id
new_table = new_table.fillna(0)
new_table.head(5)

# Select the active members
new_table = new_table.loc[new_table['active'] == 1]
new_table.head(5)

# Rename columns
new_table["first_week"] = new_table["visit_count_m4_wk"]
new_table["second_week"] = new_table["visit_count_m3_wk"]
new_table["third_week"] = new_table["visit_count_m2_wk"]
new_table["fourth_week"] = new_table["visit_count_m1_wk"]
#new_table["present_week"] = new_table["visit_count_0_wk"]
#new_table["target_week"] = new_table["visit_count_1_wk"]
new_table["first_four_weeks_mean"] = new_table["visit_count_m1_4_mean"]
new_table["first_four_weeks_std"] = new_table["visit_count_m1_4_stdev"]
new_table = new_table[["firstname","lastname","first_week","second_week","third_week","fourth_week",
               "first_four_weeks_mean","first_four_weeks_std"]]
new_table.head(5)

# Arrays of each feature
first_week = new_table.iloc[:,2].values
second_week = new_table.iloc[:,3].values
third_week = new_table.iloc[:,4].values
fourth_week = new_table.iloc[:,5].values
mean_week = new_table.iloc[:,6].values
std_week = new_table.iloc[:,7].values

# Exclude present week's beta
betas = np.delete(betas,4)
betas

# Calculate target
y = alpha + (betas[0] * first_week) + (betas[1] * second_week) + (betas[2] * third_week) + (betas[3] * fourth_week) + (betas[4] * mean_week) + (betas[5] * std_week)
#print(y)

# Sigmoid function
p = 1 / (1 + np.exp(-y))
#print(p)

# Add columns
new_table["alpha"] = alpha[0]
new_table["beta1"] = betas[0]
new_table["beta2"] = betas[1]
new_table["beta3"] = betas[2]
new_table["beta4"] = betas[3]
new_table["beta5"] = betas[4]
new_table["beta6"] = betas[5]
new_table["name"] = new_table["firstname"] +"-"+ new_table["lastname"]
new_table = new_table[["name","first_week","second_week","third_week","fourth_week","first_four_weeks_mean",
                       "first_four_weeks_std","alpha","beta1","beta2","beta3","beta4","beta5","beta6"]]
new_table.head(5)

# Create dataframe
data = pd.DataFrame({
    'id':'-',
    'name':'-',
    'alpha':new_table.alpha,
    'beta1':new_table.beta1,
    'beta2':new_table.beta2,
    'beta3':new_table.beta3,
    'beta4':new_table.beta4,
    'beta5':new_table.beta5,
    'beta6':new_table.beta6
})

data = data.drop_duplicates(subset='id', keep="first")
print(data.to_string(index=False))

# Fill ds_formulas table
import MySQLdb

conn = MySQLdb.connect(
    host='Gymlab.mysql.pythonanywhere-services.com',
    user='Gymlab',
    passwd='B@s3C@mp!',
    db='Gymlab$pydb')

cur = conn.cursor()

# creating column list for insertion
cols = "`,`".join([str(i) for i in data.columns.tolist()])

# Insert DataFrame recrds one by one.
for i,row in data.iterrows():
    sql = "INSERT INTO `ds_formulas` (`" +cols + "`) VALUES (" + "%s,"*(len(row)-1) + "%s) ON DUPLICATE KEY UPDATE name=VALUES(name), alpha=VALUES(alpha), beta1=VALUES(beta1), beta2=VALUES(beta2), beta3=VALUES(beta3), beta4=VALUES(beta4), beta5=VALUES(beta5), beta6=VALUES(beta6)"
    cur.execute(sql, tuple(row))

    # the connection is not autocommitted by default, so we must commit to save our changes
    conn.commit()