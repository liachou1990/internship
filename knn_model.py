# -*- coding: utf-8 -*-
"""knn_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a0lKnY5Qde569buLrLefG4NPW5milO0P
"""

import mysql.connector
import sshtunnel
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

sshtunnel.SSH_TIMEOUT = 5.0
sshtunnel.TUNNEL_TIMEOUT = 5.0

with sshtunnel.SSHTunnelForwarder(
    ('ssh.pythonanywhere.com'),
    ssh_username='Gymlab', ssh_password='B@s3C@mp!',
    remote_bind_address=('Gymlab.mysql.pythonanywhere-services.com', 3306)
) as tunnel:
    connection = mysql.connector.connect(
        user='Gymlab', password='B@s3C@mp!',
        host='127.0.0.1', port=tunnel.local_bind_port,
        database='Gymlab$pydb',
    )
    
    table = pd.read_sql('SELECT * FROM data_goalie_prio', connection)
    
    connection.close()

# Load first five rows
table.head(5)

# Check for duplicate rows by id
table.sort_values("id",inplace=True)
table.drop_duplicates(subset="id",keep=False,inplace=True)
del table["date_from"]
del table["date_to"]
del table["id"]
table.loc[table['gender'] == "f", 'gender'] = 1
table.loc[table['gender'] == "m", 'gender'] = 0
table.loc[table['gender'] == "M", 'gender'] = 0
table = table.fillna(0)
table.head(5)

# Move target column at the end
table = table[["member_id","yearweek","year","month","week_number","member_since_weeks","member_since_months",
               "member_since_years","age","gender","visit_count_m4_wk","visit_count_m3_wk","visit_count_m2_wk",
               "visit_count_m1_wk","visit_count_0_wk","visit_count_m1_4_wk_mean","visit_count_m1_4_wk_stdev",
               "visit_count_1_wk"]]
table.head(5)

# Rename columns
table["first_week"] = table["visit_count_m4_wk"]
table["second_week"] = table["visit_count_m3_wk"]
table["third_week"] = table["visit_count_m2_wk"]
table["fourth_week"] = table["visit_count_m1_wk"]
table["present_week"] = table["visit_count_0_wk"]
table["target_week"] = table["visit_count_1_wk"]
table["first_four_weeks_mean"] = table["visit_count_m1_4_wk_mean"]
table["first_four_weeks_std"] = table["visit_count_m1_4_wk_stdev"]
table = table[["first_week","second_week","third_week","fourth_week",
                 "present_week","first_four_weeks_mean","first_four_weeks_std","target_week"]]
table.head(5)

# Classify Target Column
def f(row):
    if row["target_week"] >= 0.5:
        val = 1
    else:
        val = 0
    return val
table["target_week"] = table.apply(f, axis=1)
table.head(5)

"""Feature selection"""

# Preprocessing
X = table.iloc[:, :-1].values
y = table.iloc[:, -1].values

# Train-test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1,stratify=y)

# Feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# Training - prediction
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=39)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
y_pred

# Evaluation
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Accuracy
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)

# Compare error rate with K
error = []

# Calculating error for K values between 1 and 40
for i in range(1, 40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error.append(np.mean(pred_i != y_test))

# Plot K vs Error
plt.figure(figsize=(12, 6))
plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')

# Commented out IPython magic to ensure Python compatibility.
# Plot K vs Accuracy
from sklearn import metrics
k_range = range(1,40)
scores = {}
scores_list = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train,y_train)
    y_pred = knn.predict(X_test)
    scores[k] = metrics.accuracy_score(y_test,y_pred)
    scores_list.append(metrics.accuracy_score(y_test,y_pred))
    
# %matplotlib inline
import matplotlib.pyplot as plt

plt.plot(k_range,scores_list)
plt.xlabel("Value of k for KNN")
plt.ylabel("Testing Accuracy")

# Use cross-validation
from sklearn.model_selection import cross_val_score

# create new knn model
knn_cv = KNeighborsClassifier(n_neighbors=5)

# train model with cv of 5
cv_scores = cross_val_score(knn_cv,X,y,cv=5)

# print each cv score and average them
print(cv_scores)
print('cv_scores mean:{}'.format(np.mean(cv_scores)))

# Hyper-tuning
from sklearn.model_selection import GridSearchCV

# create new knn model
knn2 = KNeighborsClassifier()

# create dictionary for n_neighbors' values
param_grid = {'n_neighbors':np.arange(1,40)}

# use grid search to test values
knn_gscv = GridSearchCV(knn2,param_grid,cv=5)

# fit model
knn_gscv.fit(X,y)

# Check top performing n_neighbors value
knn_gscv.best_params_

# Check mean score of the top performing value of n_neighbors
knn_gscv.best_score_